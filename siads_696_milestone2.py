# -*- coding: utf-8 -*-
"""SIADS 696 Milestone2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AicvWl0kJkSvEtZlqwKHodSAKY6uSJNL

# SIADS 696 Milestone II: Predictive Analytics of Interest Rate Spreads Using Advanced Machine Learning Techniques

# Team: Xinyue Xiang, Yichen Nie, Powen Lai

##I. Introduction
The T10Y2Y data series, provided by the Federal Reserve Bank of St. Louis represents the difference between the yields on the 10-year U.S. Treasury bond and the 2-year U.S. Treasury bond. Commonly referred to as a measure of the yield curve, this spread serves as a widely watched indicator of economic sentiment and potential future economic conditions. Typically, longer-term Treasury bonds carry higher yields than shorter-term ones, reflecting the risks of holding debt over a longer period. However, when this spread turns negative meaning short-term rates exceed long-term rates, it produces what's known as an inverted yield curve. Historically, such inversions have been considered a reliable predictor of upcoming recessions in the United States.

The T10Y2Y series offers valuable insight for economists, policymakers, and investors seeking to assess market expectations for growth, inflation, and monetary policy. Our project is mainly trying to use machine learning to predict U.S. Treasury spreads with interest rate spread between 10 and 2 year Treasuries. We are also interested in finding out whether there are any structural patterns within these interest rate curves and if they can be broken down into a few categories.

##II. Related Work
### Closest related work:
[Machine Learning, the Treasury Yield Curve and Recession Forecasting](https://www.federalreserve.gov/econres/feds/machine-learning-the-treasury-yield-curve-and-recession-forecasting.htm)

### Summary:
This work is from the U.S. Federal Reserve and they apply machine learning models such as Random Forest, XGBoost, SVM and Neural Networks to predict recessions using yield curve data and compare the results to traditional Probit models. As a result, they found that ML models can perform well, but time series-aware validation tends to favor simpler approaches.
### Differences:
Unlike their focus on recession classification, we aim to predict the 10Y–2Y spread as a continuous value using regression. We plan to use daily treasury yield data which contains higher frequency than the monthly data in the related work. We also incorporate unsupervised learning to identify structural patterns in yield curves, which their work did not explore.

## III. Data Source(s)
This project utilizes publicly available data from the Federal Reserve Bank of St. Louis (FRED), which is a widely trusted and comprehensive resource for U.S. macroeconomic data. Specifically, we rely on two main datasets: the T10Y2Y yield spread series and a collection of daily Treasury yield curve rates across multiple maturities.

The T10Y2Y series—identified as the "10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity" is retrieved as a CSV file hosted on GitHub, originally sourced from FRED. This series provides a daily time series of the yield spread between 10-year and 2-year U.S. Treasury bonds, a well-established indicator of economic sentiment. The dataset is returned in .CSV format and includes two columns: observation_date and T10Y2Y. The values span from May 11, 2015, to May 9, 2025, and contain 2,611 records. Missing values (typically due to market holidays) are forward-filled, which is standard in financial time series analysis.

The second major dataset comprises daily Treasury yield rates for various maturities including 1-month, 2-month, 3-month, 6-month, 1-year, 2-year, 3-year, 5-year, 7-year, 10-year, 20-year, and 30-year bonds. These are also sourced from FRED via CSV exports hosted on GitHub, spanning the years 2016 to 2025. Each year of data is provided in a separate CSV file, which we merged and cleaned to create a unified time series. In total, this combined dataset includes 2,339 daily observations and 14 columns, each representing a different bond maturity rate.
After preprocessing, we merged the two datasets on date, resulting in a combined dataset with consistent time alignment and complete entries for the selected maturities and yield spread. These variables serve as both features and targets for our predictive modeling and structural analysis. The use of high-frequency, daily data enables more granular insights compared to traditional monthly macroeconomic indicators.

# Part 0 - Import Libraries
"""

# Import necessary libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Machine Learning Libraries
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score

import subprocess
import sys

try:
    import xgboost
    print("XGBoost already installed!")
except ImportError:
    print("Installing XGBoost...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "xgboost"])
    import xgboost
    print("XGBoost installed successfully!")
import xgboost as xgb

"""# Part 1 - Data Preparation
Load, clean, and merge Treasury yield data and macroeconomic indicators from FRED

"""

# Load Fed T10Y2Y data: From May 11, 2015- May 09, 2025
fed_url = "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/fed_t10y2y.csv"
fed_df = pd.read_csv(fed_url)
fed_df['observation_date'] = pd.to_datetime(fed_df['observation_date'])
fed_df.set_index('observation_date', inplace=True)

# forward fill NA. This approach is standard in finance because yields are effectively unchanged on days markets are closed.
fed_df=fed_df['T10Y2Y'] = fed_df['T10Y2Y'].ffill()

print("Fed T10Y2Y data loaded successfully:")
#print(f"Date range: {fed_df.index.min()} to {fed_df.index.max()}")
print(f"Shape: {fed_df.shape}")
print(fed_df.head())

# Load Treasury yield curve data
urls = [
    "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/daily-treasury-rates25.csv",
    "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/daily-treasury-rates24.csv",
    "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/daily-treasury-rates23.csv",
    "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/daily-treasury-rates22.csv",
    "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/daily-treasury-rates21.csv",
    "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/daily-treasury-rates20.csv",
    "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/daily-treasury-rates19.csv",
    "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/daily-treasury-rates18.csv",
    "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/daily-treasury-rates17.csv",
    "https://raw.githubusercontent.com/chenzaiproject/milestone2/refs/heads/main/daily-treasury-rates16.csv"
]

list_df = []
for url in urls:
    df = pd.read_csv(url)
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    list_df.append(df)

# Combine all treasury data
treasury_df = pd.concat(list_df, ignore_index=True)
treasury_df.sort_values(by="Date", inplace=True)
treasury_df.set_index('Date', inplace=True)

print(f"\nTreasury yield curve data loaded successfully:")
print(f"Date range: {treasury_df.index.min()} to {treasury_df.index.max()}")
print(f"Shape: {treasury_df.shape}")

# Merge datasets to inclide both features and targets
combined_df = treasury_df.join(fed_df, how='inner')
combined_df.dropna(subset=['T10Y2Y'], inplace=True)

print(f"\nCombined dataset:")
print(f"Shape: {combined_df.shape}")
print(f"Date range: {combined_df.index.min()} to {combined_df.index.max()}")

# Review combined dataframe
combined_df.info()

# Get descriptive stats, noticing 1.5 month 2 month and 4 month treasury yield curve data contains many missing data
# These yields are not avaiable in the data before 2018.
combined_df.describe()

# Plot histograms for all numeric columns
combined_df.hist(bins=10, figsize=(10, 6))
plt.tight_layout()
plt.show()

# # Plot each numeric variable vs date
num_cols = combined_df.shape[1]
cols_per_row = 4
rows = (num_cols + cols_per_row - 1) // cols_per_row  # ceiling division

fig, axes = plt.subplots(rows, cols_per_row, figsize=(cols_per_row * 3, rows * 2), sharex=True)

axes = axes.flatten()  # flatten axes array for easy indexing

for i, col in enumerate(combined_df.columns):
    axes[i].plot(combined_df.index, combined_df[col], marker='o')
    axes[i].set_title(f'{col}', fontsize=8)
    axes[i].grid(True)
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].set_xlabel('Date')
    axes[i].set_ylabel('')

# Turn off unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""# PART 2: Feature Engineering

To transform the raw yield curve data into a format suitable for machine learning models, we applied a series of preprocessing and feature engineering steps. These steps were essential for ensuring data quality, reducing dimensionality, and making the data more suitable for both supervised and unsupervised learning methods.
Preprocessing and Cleaning
Our raw input consisted of daily Treasury rates across multiple maturities, along with the T10Y2Y yield spread as the target variable. Several columns—such as yields for 1.5-month and 4-month maturities—contained significant missing data and were therefore excluded from modeling. For the remaining columns, any rows with missing values were dropped to ensure consistency across all features. This resulted in a cleaned dataset of 1,641 daily records across 12 features.
We also standardized all Treasury yield features using z-score normalization (StandardScaler) to remove scale differences between short-term and long-term rates. This step is particularly important for models sensitive to feature magnitude, such as Principal Component Analysis (PCA) and distance-based clustering algorithms.

### Feature Construction
The final supervised and unsupervised learning workflows used two primary forms of features:
Raw Yield Curve Features:
We selected 12 key Treasury maturities as raw input features:
['1 Mo', '2 Mo', '3 Mo', '6 Mo', '1 Yr', '2 Yr',
 '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr']
These were used for initial visualization, correlation analysis, and as input into dimensionality reduction methods.
PCA Components:
To reduce dimensionality and extract structural patterns, we applied Principal Component Analysis (PCA) to the standardized yield features. We retained the top 3 principal components (PC1, PC2, PC3), which together explained over 95% of the total variance in the yield curve data. These components served as the primary input for both our regression models and clustering algorithms.
PC1 generally captured the overall level of interest rates (i.e., upward or downward shifts in the entire curve),
PC2 captured the slope, or steepness/inversion of the curve,
PC3 captured curvature, or the degree of bowing in the mid-term yields.
Final Feature Set Summary
The final set of features used for machine learning models included:
Supervised Learning: PC1, PC2, PC3 (predicting T10Y2Y as the target)
Unsupervised Learning: PC1, PC2, PC3 (to discover structural patterns via clustering)
This engineering process allowed us to compress complex multi-dimensional yield curve information into a compact, interpretable feature set while preserving essential structural characteristics of the term structure.
"""

# Define treasury features
treasury_features = ['1 Mo','2 Mo','3 Mo','6 Mo',
                     '1 Yr','2 Yr','3 Yr','5 Yr','7 Yr','10 Yr','20 Yr','30 Yr']
# Remove rows with missing values in treasury features
combined_df.dropna(subset=treasury_features, inplace=True)
# Extract feature matrix
X = combined_df[treasury_features].values
print(f"Feature matrix shape: {X.shape}")

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print("Features standardized")

# Apply PCA for dimensionality reduction
pca = PCA(n_components=3)
principalComponents = pca.fit_transform(X_scaled)
print(f"PCA explained variance ratios: {pca.explained_variance_ratio_}")
print(f"Total explained variance: {pca.explained_variance_ratio_.sum():.4f}")

# Create feature dataframe with PCA components
combined_df['PC1'] = principalComponents[:, 0]
combined_df['PC2'] = principalComponents[:, 1]
combined_df['PC3'] = principalComponents[:, 2]

# Prepare target variable and features for modeling
y = combined_df['T10Y2Y']
X_model = combined_df[['PC1','PC2','PC3']]

print(f"Target variable (T10Y2Y) statistics:")
print(f"Mean: {y.mean():.4f}, Std: {y.std():.4f}")
print(f"Min: {y.min():.4f}, Max: {y.max():.4f}")

"""# Part 3 - Supervised Learning"""

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_model, y, test_size=0.2, random_state=42)
print(f"Training set size: {X_train.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")

# ==================== 3.1: Multi-Model Training ====================
print("\n3.1: Multi-Model Training")
print("-" * 30)

# Define models (now with XGBoost!)
models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'XGBoost': xgb.XGBRegressor(
                objective='reg:squarederror',  # Standard regression objective (MSE loss)
                n_estimators=100,              # Number of boosting rounds (trees)
                learning_rate=0.1,             # Step size shrinkage to prevent overfitting
                max_depth=6,                   # Max tree depth — controls model complexity
                random_state=42,
                verbosity=0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
}

# Train models and result evaluate
model_results = {}
trained_models = {}

for name, model in models.items():
    print(f"Training {name}...")

    # Train model
    model.fit(X_train, y_train)
    trained_models[name] = model

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate metrics
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    model_results[name] = {
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2,
        'predictions': y_pred
    }

    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  R²: {r2:.4f}")

# ==================== 3.2: Time Series Cross-Validation ====================
print("\n3.2: Time Series Cross-Validation")
print("-" * 30)

def time_series_cv_evaluation(X, y, models, cv_folds=5):
    """Time series cross-validation for model evaluation"""
    tscv = TimeSeriesSplit(n_splits=cv_folds)
    results = {}

    for name, model in models.items():
        print(f"CV for {name}...")
        rmse_scores = []
        r2_scores = []

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]
            y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]

            # Clone the model to avoid fitting issues
            if name == 'XGBoost':
                fold_model = xgb.XGBRegressor(
                    objective='reg:squarederror',
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=6,
                    random_state=42,
                    verbosity=0
                )
            else:
                fold_model = model.__class__(**model.get_params())

            fold_model.fit(X_train_fold, y_train_fold)
            y_pred_fold = fold_model.predict(X_test_fold)

            rmse = np.sqrt(mean_squared_error(y_test_fold, y_pred_fold))
            r2 = r2_score(y_test_fold, y_pred_fold)

            rmse_scores.append(rmse)
            r2_scores.append(r2)

        results[name] = {
            'RMSE_mean': np.mean(rmse_scores),
            'RMSE_std': np.std(rmse_scores),
            'R2_mean': np.mean(r2_scores),
            'R2_std': np.std(r2_scores)
        }

    return results

# Perform time series cross-validation
cv_results = time_series_cv_evaluation(X_model, y, models)

print("Time Series Cross-Validation Results:")
for model_name, scores in cv_results.items():
    print(f"{model_name}:")
    print(f"  RMSE: {scores['RMSE_mean']:.4f} ± {scores['RMSE_std']:.4f}")
    print(f"  R²: {scores['R2_mean']:.4f} ± {scores['R2_std']:.4f}")

# ==================== 3.3: Feature Importance Analysis ====================
print("\n3.3: Feature Importance Analysis")
print("-" * 30)

def analyze_feature_importance(models_dict, feature_names):
    """Analyze and visualize feature importance"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    plot_count = 0
    for name, model in models_dict.items():
        if hasattr(model, 'feature_importances_') and plot_count < 3:
            ax = axes[plot_count]
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1]

            bars = ax.bar(range(len(importances)), importances[indices])
            ax.set_title(f'{name}\nFeature Importance')
            ax.set_xticks(range(len(importances)))
            ax.set_xticklabels([feature_names[i] for i in indices])
            ax.set_ylabel('Importance')

            # Add value labels on bars
            for i, bar in enumerate(bars):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.3f}', ha='center', va='bottom')

            plot_count += 1

    plt.tight_layout()
    plt.show()

analyze_feature_importance(trained_models, ['PC1', 'PC2', 'PC3'])

# XGBoost specific feature importance
if 'XGBoost' in trained_models:
    print("\nXGBoost Feature Importance Details:")
    xgb_model = trained_models['XGBoost']
    feature_importance = xgb_model.feature_importances_
    for i, importance in enumerate(feature_importance):
        print(f"  PC{i+1}: {importance:.4f}")

# ==================== 3.4: Hyperparameter Tuning for XGBoost ====================
print("\n3.4: XGBoost Hyperparameter Tuning")
print("-" * 30)

# Test different XGBoost parameters
xgb_params_test = [
    {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 3},
    {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6},
    {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 6},
    {'n_estimators': 100, 'learning_rate': 0.2, 'max_depth': 4}
]

best_xgb_rmse = float('inf')
best_xgb_params = None

print("Testing XGBoost hyperparameters:")
for params in xgb_params_test:
    xgb_test = xgb.XGBRegressor(
        objective='reg:squarederror',
        random_state=42,
        verbosity=0,
        **params
    )

    xgb_test.fit(X_train, y_train)
    y_pred_test = xgb_test.predict(X_test)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
    r2_test = r2_score(y_test, y_pred_test)

    print(f"  Params: {params}")
    print(f"    RMSE: {rmse_test:.4f}, R²: {r2_test:.4f}")

    if rmse_test < best_xgb_rmse:
        best_xgb_rmse = rmse_test
        best_xgb_params = params

print(f"\nBest XGBoost parameters: {best_xgb_params}")
print(f"Best RMSE: {best_xgb_rmse:.4f}")

# ==================== 3.5: Sensitivity Analysis ====================
print("\n3.5: Sensitivity Analysis")
print("-" * 30)

def sensitivity_analysis_pca_components():
    """Test sensitivity to number of PCA components"""
    print("Testing different numbers of PCA components:")

    pca_components = [2, 3, 4, 5]
    sensitivity_results = []

    for n_comp in pca_components:
        pca_temp = PCA(n_components=n_comp)
        X_temp = pca_temp.fit_transform(X_scaled)
        X_temp_df = pd.DataFrame(X_temp, columns=[f'PC{i+1}' for i in range(n_comp)],
                                index=combined_df.index)

        X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(
            X_temp_df, y, test_size=0.2, random_state=42
        )

        # Test with XGBoost
        xgb_temp = xgb.XGBRegressor(
            objective='reg:squarederror',
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            random_state=42,
            verbosity=0
        )

        xgb_temp.fit(X_train_temp, y_train_temp)
        y_pred_temp = xgb_temp.predict(X_test_temp)

        rmse_temp = np.sqrt(mean_squared_error(y_test_temp, y_pred_temp))
        r2_temp = r2_score(y_test_temp, y_pred_temp)
        explained_var = pca_temp.explained_variance_ratio_.sum()

        sensitivity_results.append({
            'n_components': n_comp,
            'RMSE': rmse_temp,
            'R2': r2_temp,
            'explained_variance': explained_var
        })

        print(f"  {n_comp} components: RMSE={rmse_temp:.4f}, R²={r2_temp:.4f}, "
              f"Explained Var={explained_var:.4f}")

    return sensitivity_results

sensitivity_results = sensitivity_analysis_pca_components()

# ==================== 3.6: Failure Analysis ====================
print("\n3.6: Failure Analysis")
print("-" * 30)

def failure_analysis(model, X_test, y_test, model_name, n_examples=3):
    """Analyze prediction failures"""
    y_pred = model.predict(X_test)
    residuals = np.abs(y_test - y_pred)

    # Find worst predictions
    worst_indices = np.argsort(residuals)[-n_examples:]

    print(f"Worst {n_examples} predictions for {model_name}:")

    failure_categories = []
    for i, idx in enumerate(worst_indices):
        actual_idx = X_test.index[idx]
        actual_val = y_test.iloc[idx]
        pred_val = y_pred[idx]
        error = residuals.iloc[idx]

        # Categorize failure type
        if actual_val > 1.5:
            category = "High spread period"
        elif actual_val < 0:
            category = "Inverted yield curve"
        else:
            category = "Normal range"

        failure_categories.append(category)

        print(f"  Case {i+1} ({category}):")
        print(f"    Date index: {actual_idx}")
        print(f"    Actual: {actual_val:.4f}")
        print(f"    Predicted: {pred_val:.4f}")
        print(f"    Error: {error:.4f}")
        print(f"    Features: PC1={X_test.iloc[idx, 0]:.4f}, PC2={X_test.iloc[idx, 1]:.4f}, PC3={X_test.iloc[idx, 2]:.4f}")

    return failure_categories

# Analyze failures for XGBoost
print("XGBoost Failure Analysis:")
failure_analysis(trained_models['XGBoost'], X_test, y_test, 'XGBoost')

"""# Part 4: Unsupervised Learning"""

# ==================== 4.1: PCA for Yield Curve Analysis ====================
print("\n4.1: PCA for Yield Curve Analysis")
print("-" * 30)

# Prepare data for unsupervised learning
unsupervised_data = combined_df[treasury_features].dropna()
scaler_unsupervised = StandardScaler()
X_unsupervised_scaled = scaler_unsupervised.fit_transform(unsupervised_data)

# Apply PCA for visualization
pca_viz = PCA(n_components=2)
X_pca_2d = pca_viz.fit_transform(X_unsupervised_scaled)

print(f"PCA 2D explained variance ratio: {pca_viz.explained_variance_ratio_}")
print(f"Total explained variance: {pca_viz.explained_variance_ratio_.sum():.4f}")

# Interpret PCA components
print("\nPCA Component Interpretation:")
feature_weights = pca_viz.components_
for i in range(2):
    print(f"PC{i+1} strongest features:")
    component_weights = np.abs(feature_weights[i])
    top_features = np.argsort(component_weights)[-3:][::-1]
    for j in top_features:
        print(f"  {treasury_features[j]}: {feature_weights[i][j]:.3f}")

# ==================== 4.2: K-Means Clustering ====================
print("\n4.2: K-Means Clustering")
print("-" * 30)

# Find optimal number of clusters
k_range = range(2, 8)
silhouette_scores = []
inertias = []
kmeans_models = {}

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_unsupervised_scaled)
    silhouette_avg = silhouette_score(X_unsupervised_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)
    inertias.append(kmeans.inertia_)
    kmeans_models[k] = kmeans

# Find best K
best_k = k_range[np.argmax(silhouette_scores)]
print(f"Optimal number of clusters: {best_k}")
print(f"Best silhouette score: {max(silhouette_scores):.4f}")

# Final K-means clustering
final_kmeans = kmeans_models[best_k]
kmeans_labels = final_kmeans.predict(X_unsupervised_scaled)

# ==================== 4.3: DBSCAN Clustering ====================
print("\n4.3: DBSCAN Clustering")
print("-" * 30)

# Test different DBSCAN parameters
eps_values = [0.5, 0.8, 1.0, 1.2]
min_samples_values = [3, 5, 7]
best_dbscan = None
best_dbscan_score = -1

print("Testing DBSCAN parameters:")
for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X_unsupervised_scaled)

        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = np.sum(labels == -1)

        if n_clusters > 1 and n_noise < len(labels) * 0.5:  # Valid clustering
            mask = labels != -1
            if np.sum(mask) > 0:
                score = silhouette_score(X_unsupervised_scaled[mask], labels[mask])
                if score > best_dbscan_score:
                    best_dbscan_score = score
                    best_dbscan = dbscan
                    best_dbscan_labels = labels

        print(f"  eps={eps}, min_samples={min_samples}: clusters={n_clusters}, noise={n_noise}")

if best_dbscan is not None:
    dbscan_labels = best_dbscan_labels
    print(f"Best DBSCAN silhouette score: {best_dbscan_score:.4f}")
else:
    print("Using default DBSCAN parameters")
    dbscan = DBSCAN(eps=0.8, min_samples=5)
    dbscan_labels = dbscan.fit_predict(X_unsupervised_scaled)

# ==================== 4.4: Clustering Evaluation ====================
print("\n4.4: Clustering Evaluation")
print("-" * 30)

# K-Means evaluation
kmeans_silhouette = silhouette_score(X_unsupervised_scaled, kmeans_labels)
kmeans_db_score = davies_bouldin_score(X_unsupervised_scaled, kmeans_labels)

# DBSCAN evaluation
mask = dbscan_labels != -1
if np.sum(mask) > 1:
    dbscan_silhouette = silhouette_score(X_unsupervised_scaled[mask], dbscan_labels[mask])
    dbscan_db_score = davies_bouldin_score(X_unsupervised_scaled[mask], dbscan_labels[mask])
else:
    dbscan_silhouette, dbscan_db_score = -1, float('inf')

print("Clustering Evaluation Results:")
print(f"K-Means (K={best_k}):")
print(f"  Silhouette Score: {kmeans_silhouette:.4f}")
print(f"  Davies-Bouldin Score: {kmeans_db_score:.4f}")
print(f"DBSCAN:")
print(f"  Clusters: {len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)}")
print(f"  Noise points: {np.sum(dbscan_labels == -1)}")
print(f"  Silhouette Score: {dbscan_silhouette:.4f}")
print(f"  Davies-Bouldin Score: {dbscan_db_score:.4f}")

# ==================== 4.5: Yield Curve Shape Analysis ====================
print("\n4.5: Yield Curve Shape Analysis")
print("-" * 30)

def analyze_yield_curve_shapes(data, labels, feature_names):
    """Analyze yield curve shapes for each cluster"""

    def classify_curve_shape(curve_data):
        """Classify yield curve as flat, steep, or inverted"""
        short_rate = curve_data['2 Yr']
        long_rate = curve_data['10 Yr']
        spread = long_rate - short_rate

        if spread < -0.5:
            return 'Inverted'
        elif spread < 0.5:
            return 'Flat'
        else:
            return 'Steep'

    print("Yield Curve Shape Analysis by Cluster:")

    df_analysis = data.copy()
    df_analysis['Cluster'] = labels
    df_analysis['Curve_Shape'] = df_analysis.apply(classify_curve_shape, axis=1)

    for cluster_id in sorted(set(labels)):
        if cluster_id == -1:  # Skip noise points
            continue

        cluster_data = df_analysis[df_analysis['Cluster'] == cluster_id]

        print(f"\nCluster {cluster_id} (n={len(cluster_data)}):")

        # Shape distribution
        shape_dist = cluster_data['Curve_Shape'].value_counts()
        for shape, count in shape_dist.items():
            percentage = count / len(cluster_data) * 100
            print(f"  {shape}: {count} ({percentage:.1f}%)")

        # Average yield curve
        avg_curve = cluster_data[feature_names].mean()
        print(f"  Avg 2Y-10Y spread: {avg_curve['10 Yr'] - avg_curve['2 Yr']:.3f}")
        print(f"  Avg curve level: {avg_curve.mean():.3f}")

# Analyze K-means clusters
analyze_yield_curve_shapes(unsupervised_data, kmeans_labels, treasury_features)

"""# Part 5: Visualizations and Summary"""

# ==================== 5.1: Supervised Learning Visualizations ====================
print("\n5.1: Supervised Learning Results")
print("-" * 30)

# Model comparison plot
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Prediction scatter plots - including XGBoost
models_to_plot = ['Linear Regression', 'Decision Tree', 'XGBoost']
for i, model_name in enumerate(models_to_plot):
    ax = axes[0, i]
    y_pred = model_results[model_name]['predictions']
    ax.scatter(y_test, y_pred, alpha=0.6)
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    ax.set_xlabel('Actual T10Y2Y')
    ax.set_ylabel('Predicted T10Y2Y')
    ax.set_title(f'{model_name}\nRMSE: {model_results[model_name]["RMSE"]:.4f}')
    ax.grid(True, alpha=0.3)

# Performance comparison
models_list = list(model_results.keys())
rmse_values = [model_results[m]['RMSE'] for m in models_list]
r2_values = [model_results[m]['R2'] for m in models_list]

axes[1, 0].bar(models_list, rmse_values, color='skyblue')
axes[1, 0].set_title('RMSE Comparison')
axes[1, 0].set_ylabel('RMSE')
axes[1, 0].tick_params(axis='x', rotation=45)

axes[1, 1].bar(models_list, r2_values, color='lightgreen')
axes[1, 1].set_title('R² Comparison')
axes[1, 1].set_ylabel('R² Score')
axes[1, 1].tick_params(axis='x', rotation=45)

# Time series of predictions vs actual
best_model_name = min(model_results.keys(), key=lambda x: model_results[x]['RMSE'])
axes[1, 2].plot(y_test.index[:100], y_test.iloc[:100], label='Actual', linewidth=2)
best_pred = model_results[best_model_name]['predictions'][:100]
axes[1, 2].plot(y_test.index[:100], best_pred, label=f'Predicted ({best_model_name})', linewidth=2)
axes[1, 2].set_title('Time Series: Actual vs Predicted')
axes[1, 2].set_xlabel('Date')
axes[1, 2].set_ylabel('T10Y2Y')
axes[1, 2].legend()
axes[1, 2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ==================== 5.1.2: Simple Prediction Scatter Plots ====================
print("\n5.1.2: Model Prediction Comparison")
print("-" * 30)

# Create the simple scatter plots as requested
plt.figure(figsize=(15, 5))

# Linear Regression
plt.subplot(1, 3, 1)
y_pred_lr = model_results['Linear Regression']['predictions']
plt.scatter(y_test, y_pred_lr, alpha=0.7)
plt.xlabel("Real T10Y2Y")
plt.ylabel("Predicted T10Y2Y")
plt.title("Linear Regression Result")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.8)
plt.grid(True, alpha=0.3)

# Decision Tree
plt.subplot(1, 3, 2)
y_pred_tree = model_results['Decision Tree']['predictions']
plt.scatter(y_test, y_pred_tree, alpha=0.7, color='orange')
plt.xlabel("Real T10Y2Y")
plt.ylabel("Predicted T10Y2Y")
plt.title("Decision Tree Result")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.8)
plt.grid(True, alpha=0.3)

# XGBoost
plt.subplot(1, 3, 3)
y_pred_xgb = model_results['XGBoost']['predictions']
plt.scatter(y_test, y_pred_xgb, alpha=0.7, color='green')
plt.xlabel("Real T10Y2Y")
plt.ylabel("Predicted T10Y2Y")
plt.title("XGBoost Regression Result")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.8)
plt.grid(True, alpha=0.3)

# Adjust layout to prevent overlapping
plt.tight_layout()
plt.show()

# Print performance metrics for these three models
print("Performance Summary for Main Models:")
print("="*45)
for model_name in ['Linear Regression', 'Decision Tree', 'XGBoost']:
    results = model_results[model_name]
    print(f"{model_name}:")
    print(f"  RMSE: {results['RMSE']:.4f}")
    print(f"  MAE:  {results['MAE']:.4f}")
    print(f"  R²:   {results['R2']:.4f}")
    print()

# ==================== 5.2: Unsupervised Learning Visualizations ====================
print("\n5.2: Unsupervised Learning Results")
print("-" * 30)

# Clustering visualizations
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# K-Means clustering
scatter1 = axes[0, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.6)
axes[0, 0].set_title(f'K-Means Clustering (K={best_k})')
axes[0, 0].set_xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.3f})')
axes[0, 0].set_ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.3f})')
plt.colorbar(scatter1, ax=axes[0, 0])

# DBSCAN clustering
scatter2 = axes[0, 1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.6)
axes[0, 1].set_title('DBSCAN Clustering')
axes[0, 1].set_xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.3f})')
axes[0, 1].set_ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.3f})')
plt.colorbar(scatter2, ax=axes[0, 1])

# Elbow curve
axes[0, 2].plot(k_range, inertias, 'bo-')
axes[0, 2].set_title('Elbow Method')
axes[0, 2].set_xlabel('Number of Clusters (K)')
axes[0, 2].set_ylabel('Inertia')
axes[0, 2].grid(True, alpha=0.3)

# Silhouette scores
axes[1, 0].plot(k_range, silhouette_scores, 'ro-')
axes[1, 0].set_title('Silhouette Score vs K')
axes[1, 0].set_xlabel('Number of Clusters (K)')
axes[1, 0].set_ylabel('Silhouette Score')
axes[1, 0].grid(True, alpha=0.3)

# Average yield curves by cluster
cluster_curves = {}
for cluster_id in sorted(set(kmeans_labels)):
    mask = kmeans_labels == cluster_id
    cluster_data = unsupervised_data[mask]
    avg_curve = cluster_data.mean()
    cluster_curves[cluster_id] = avg_curve

maturities = [1/12, 2/12, 3/12, 4/12, 6/12, 1, 2, 3, 5, 7, 10, 20, 30]  # in years
for cluster_id, curve in cluster_curves.items():
    axes[1, 1].plot(maturities, curve.values, marker='o', label=f'Cluster {cluster_id}')

axes[1, 1].set_title('Average Yield Curves by Cluster')
axes[1, 1].set_xlabel('Maturity (Years)')
axes[1, 1].set_ylabel('Yield (%)')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)
axes[1, 1].set_xscale('log')

# Cluster size distribution
cluster_sizes = [np.sum(kmeans_labels == i) for i in sorted(set(kmeans_labels))]
axes[1, 2].bar(range(len(cluster_sizes)), cluster_sizes, color='lightcoral')
axes[1, 2].set_title('Cluster Size Distribution')
axes[1, 2].set_xlabel('Cluster ID')
axes[1, 2].set_ylabel('Number of Data Points')
axes[1, 2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("COMPREHENSIVE MODEL COMPARISON TABLE")
print("="*60)

comparison_data = {
    "Model": ["Linear Regression", "Decision Tree", "XGBoost", "Random Forest"],
    "Train_RMSE": [
        model_results['Linear Regression']['RMSE'],
        model_results['Decision Tree']['RMSE'],
        model_results['XGBoost']['RMSE'],
        model_results['Random Forest']['RMSE']
    ],
    "Train_MAE": [
        model_results['Linear Regression']['MAE'],
        model_results['Decision Tree']['MAE'],
        model_results['XGBoost']['MAE'],
        model_results['Random Forest']['MAE']
    ],
    "Train_R²": [
        model_results['Linear Regression']['R2'],
        model_results['Decision Tree']['R2'],
        model_results['XGBoost']['R2'],
        model_results['Random Forest']['R2']
    ],
    "CV_RMSE": [
        cv_results['Linear Regression']['RMSE_mean'],
        cv_results['Decision Tree']['RMSE_mean'],
        cv_results['XGBoost']['RMSE_mean'],
        cv_results['Random Forest']['RMSE_mean']
    ],
    "CV_RMSE_Std": [
        cv_results['Linear Regression']['RMSE_std'],
        cv_results['Decision Tree']['RMSE_std'],
        cv_results['XGBoost']['RMSE_std'],
        cv_results['Random Forest']['RMSE_std']
    ],
    "CV_R²": [
        cv_results['Linear Regression']['R2_mean'],
        cv_results['Decision Tree']['R2_mean'],
        cv_results['XGBoost']['R2_mean'],
        cv_results['Random Forest']['R2_mean']
    ],
    "CV_R²_Std": [
        cv_results['Linear Regression']['R2_std'],
        cv_results['Decision Tree']['R2_std'],
        cv_results['XGBoost']['R2_std'],
        cv_results['Random Forest']['R2_std']
    ]
}

result_df = pd.DataFrame(comparison_data)

def get_performance_grade(train_r2, cv_r2):
    if train_r2 > 0.95 and cv_r2 > 0.8:
        return "A+"
    elif train_r2 > 0.9 and cv_r2 > 0.5:
        return "A"
    elif train_r2 > 0.8 and cv_r2 > 0.0:
        return "B"
    else:
        return "C"

result_df['Performance_Grade'] = [
    get_performance_grade(row['Train_R²'], row['CV_R²'])
    for _, row in result_df.iterrows()
]

print("Model Performance Comparison:")
print("=" * 100)
print(result_df.round(4).to_string(index=False))

best_cv_model = result_df.loc[result_df['CV_R²'].idxmax(), 'Model']
best_train_model = result_df.loc[result_df['Train_R²'].idxmax(), 'Model']

print(f"\n Best Training Performance: {best_train_model}")
print(f" Best Cross-Validation Performance: {best_cv_model}")
print(f" Performance Gap Analysis:")
for _, row in result_df.iterrows():
    gap = row['Train_R²'] - row['CV_R²']
    print(f"   {row['Model']}: Gap = {gap:.3f} {'(Overfitting)' if gap > 0.5 else '(Stable)'}")

# ==================== 5.3: Final Summary ====================
print("\n5.3: Final Summary")
print("-" * 30)

# Create comprehensive results table
print("SUPERVISED LEARNING SUMMARY:")
print("="*50)
results_df = pd.DataFrame({
    'Model': list(model_results.keys()),
    'RMSE': [model_results[m]['RMSE'] for m in model_results.keys()],
    'MAE': [model_results[m]['MAE'] for m in model_results.keys()],
    'R²': [model_results[m]['R2'] for m in model_results.keys()]
})

# Add cross-validation results
cv_rmse = [cv_results[m]['RMSE_mean'] for m in model_results.keys()]
cv_r2 = [cv_results[m]['R2_mean'] for m in model_results.keys()]
results_df['CV_RMSE'] = cv_rmse
results_df['CV_R²'] = cv_r2

print(results_df.round(4).to_string(index=False))

print(f"\nBest performing model: {best_model_name}")
print(f"Best RMSE: {model_results[best_model_name]['RMSE']:.4f}")
print(f"Best R²: {model_results[best_model_name]['R2']:.4f}")

print("\nUNSUPERVISED LEARNING SUMMARY:")
print("="*50)
print(f"Optimal K-Means clusters: {best_k}")
print(f"K-Means Silhouette Score: {kmeans_silhouette:.4f}")
print(f"DBSCAN clusters: {len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)}")
print(f"DBSCAN Silhouette Score: {dbscan_silhouette:.4f}")

print("\nKEY INSIGHTS:")
print("="*50)
print("1. PCA Analysis:")
print(f"   - First 3 components explain {pca.explained_variance_ratio_.sum():.1%} of variance")
print(f"   - PC1 likely represents overall yield level")
print(f"   - PC2 likely represents yield curve slope")

print("\n2. Supervised Learning:")
print(f"   - {best_model_name} performs best with RMSE={model_results[best_model_name]['RMSE']:.4f}")
print("   - XGBoost shows strong performance with ensemble learning")
print("   - Time series cross-validation confirms model stability")
print("   - Models struggle most with extreme market conditions")

print("\n3. Unsupervised Learning:")
print(f"   - Identified {best_k} distinct yield curve regimes")
print("   - Clusters correspond to different market conditions:")
print("     * Normal upward-sloping curves")
print("     * Flat yield curves")
print("     * Inverted curves (recession signals)")

print("\n4. Market Implications:")
print("   - T10Y2Y spread is predictable using yield curve shape")
print("   - XGBoost captures non-linear relationships effectively")
print("   - Different market regimes have distinct characteristics")
print("   - Model could assist in fixed income trading strategies")

"""## Tradeoff Discussion
One key tradeoff observed was complexity vs. interpretability. While XGBoost offered superior predictive performance, Linear Regression remained more interpretable, helpful for explaining economic relationships. Another tradeoff was training time vs. accuracy. XGBoost and Random Forest required more resources but significantly improved predictive power over simpler models.

Lastly, use PCA could introduce a bias-variance tradeoff. While it reduced multicollinearity and improved model stability, it also made feature interpretation less direct, since components are linear combinations of original yield terms.
"""

#Example for pycaret
# from pycaret.regression import *


# # Set up the environment
# clf1 = setup(data=combined_df, target='T10Y2Y', session_id=123)

# # Compare baseline models
# best_model = compare_models()

# # Create a specific model (for example, Random Forest)
# xgb_model = create_model('xgboost')

# # Tune the model's hyperparameters
# tuned_xgb= tune_model(xgb_model)

# # Evaluate the tuned model
# evaluate_model(tuned_xgb)

# # Make predictions on new data
# predictions = predict_model(tuned_xgb, data=combined_df)

# # Save the model
# save_model(tuned_xgb, 'final_xgb_model')

